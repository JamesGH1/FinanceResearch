{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a6c179",
   "metadata": {},
   "source": [
    "# Index Creation, Charting, Simple Statistical Analysis and Connectedness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe6af2f",
   "metadata": {},
   "source": [
    "## Part 1: Loading the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d2f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates # For better date formatting\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bbb4fb",
   "metadata": {},
   "source": [
    "## Part 2: Functions for loading in the prices from Refinitiv for each stock and to calculate our index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e08126",
   "metadata": {},
   "source": [
    "### Function 2.A: Load and Prepare Refinitiv obtained company price data from each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff82ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(filepath, sheet_name=0):\n",
    "    \"\"\"\n",
    "    Loads data from a single Excel file, and prepares it.\n",
    "    Each pair of (Exchange Date, Price) columns is treated as a separate company.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        xls = pd.ExcelFile(filepath)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return pd.DataFrame(), [] # Return empty DataFrame and list\n",
    "    df_raw = pd.read_excel(xls, sheet_name=sheet_name, header=0)\n",
    "\n",
    "    all_company_data = []\n",
    "    company_names = []\n",
    "\n",
    "    if df_raw.shape[1] % 2 != 0:\n",
    "        print(f\"Warning: The Excel file {filepath} has an odd number of columns. Expecting Date/Price pairs.\")\n",
    "        # Process pairs and ignore the last column if odd.\n",
    "\n",
    "    # Iterate through columns ensuring we don't go out of bounds for price_col_name\n",
    "    for i in range(0, df_raw.shape[1] - (df_raw.shape[1] % 2) , 2):\n",
    "        date_col_name = df_raw.columns[i]\n",
    "        price_col_name = df_raw.columns[i+1]\n",
    "        company_name = price_col_name # Assuming price column header is the company name\n",
    "\n",
    "        company_df = df_raw[[date_col_name, price_col_name]].copy()\n",
    "        company_df.columns = ['Date', 'Price'] # Standardize column names\n",
    "\n",
    "        # Convert 'Date' column to datetime objects, handling potential errors\n",
    "        company_df['Date'] = pd.to_datetime(company_df['Date'], errors='coerce')\n",
    "        company_df.dropna(subset=['Date'], inplace=True) # Remove rows where date conversion failed\n",
    "\n",
    "        # Remove rows where Price might be missing (e.g., before IPO) or non-numeric\n",
    "        company_df['Price'] = pd.to_numeric(company_df['Price'], errors='coerce')\n",
    "        company_df.dropna(subset=['Price'], inplace=True)\n",
    "\n",
    "        if company_df.empty:\n",
    "            continue\n",
    "\n",
    "        company_df.set_index('Date', inplace=True)\n",
    "        company_df.rename(columns={'Price': company_name}, inplace=True)\n",
    "\n",
    "        if not company_df.empty:\n",
    "            all_company_data.append(company_df)\n",
    "            company_names.append(company_name)\n",
    "\n",
    "    if not all_company_data:\n",
    "        return pd.DataFrame(), []\n",
    "\n",
    "    merged_df = pd.concat(all_company_data, axis=1, join='outer')\n",
    "    merged_df.sort_index(inplace=True)\n",
    "    return merged_df, company_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6adf05",
   "metadata": {},
   "source": [
    "### Function 2.B: Calculating the Equally Weighted Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb73f61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_equal_weighted_index(price_data_df, base_value=100):\n",
    "    \"\"\"\n",
    "    Calculates an equal-weighted index.\n",
    "    \"\"\"\n",
    "    if price_data_df.empty:\n",
    "        return pd.Series(dtype=float), pd.DataFrame(dtype=float), pd.DataFrame(dtype=float), pd.Series(dtype=int)\n",
    "\n",
    "    filled_price_data = price_data_df.ffill()\n",
    "    daily_returns = filled_price_data.pct_change()\n",
    "    num_companies_active = filled_price_data.notna().sum(axis=1)\n",
    "    index_daily_returns = daily_returns.mean(axis=1)\n",
    "\n",
    "    first_valid_date = None\n",
    "    if not num_companies_active[num_companies_active > 0].empty:\n",
    "        first_valid_date = num_companies_active[num_companies_active > 0].index[0]\n",
    "        index_daily_returns.loc[first_valid_date] = 0\n",
    "    else:\n",
    "        return pd.Series(dtype=float), daily_returns, filled_price_data, num_companies_active\n",
    "\n",
    "    index_values = base_value * (1 + index_daily_returns).cumprod()\n",
    "    index_values = index_values.reindex(filled_price_data.index)\n",
    "\n",
    "    if first_valid_date:\n",
    "        if not index_values.empty and index_values.index[0] < first_valid_date:\n",
    "             index_values.loc[:pd.Timestamp(first_valid_date) - pd.Timedelta(days=1)] = float('nan')\n",
    "        if first_valid_date in index_values.index:\n",
    "            index_values.loc[first_valid_date] = base_value\n",
    "    else:\n",
    "        index_values[:] = float('nan')\n",
    "\n",
    "    return index_values, daily_returns, filled_price_data, num_companies_active"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bf0ca1",
   "metadata": {},
   "source": [
    "### Function 2.C: Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f806f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_return_index(file_path_to_process, index_name_prefix):\n",
    "    \"\"\"\n",
    "    Loads data, calculates index, and returns it for later use.\n",
    "    This version is simplified to only return the index series.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Processing {index_name_prefix}: {file_path_to_process} ---\")\n",
    "    combined_prices_df, _ = load_and_prepare_data(file_path_to_process)\n",
    "\n",
    "    if not combined_prices_df.empty:\n",
    "        index_series, _, _, _ = calculate_equal_weighted_index(combined_prices_df, base_value=100)\n",
    "        if not index_series.dropna().empty:\n",
    "            print(f\"Successfully calculated {index_name_prefix} Index.\")\n",
    "            return index_series.rename(f\"{index_name_prefix} Index\") # Rename for clarity\n",
    "        else:\n",
    "            print(f\"Index calculation for {index_name_prefix} resulted in no valid data.\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"No data loaded from {file_path_to_process}.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16332a6",
   "metadata": {},
   "source": [
    "### Function 2.D: Plotting all the indexes in a single chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f362ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_indices(indices_df, title='Comparative Index Performance'):\n",
    "    \"\"\"\n",
    "    Plots multiple index series from a DataFrame on the same monthly chart.\n",
    "    \"\"\"\n",
    "    if indices_df.empty:\n",
    "        print(\"DataFrame is empty. Cannot plot.\")\n",
    "        return\n",
    "\n",
    "    # Resample to monthly, using the last value of the month\n",
    "    monthly_df = indices_df.resample('M').last().dropna(how='all')\n",
    "\n",
    "    if monthly_df.empty:\n",
    "        print(\"No data available for monthly plotting after resampling.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    for column in monthly_df.columns:\n",
    "        plt.plot(monthly_df.index, monthly_df[column], marker='o', linestyle='-', label=column)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Index Value')\n",
    "    plt.grid(True)\n",
    "    plt.legend() # Add a legend to distinguish the lines\n",
    "\n",
    "    # Formatting the x-axis for dates\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    locator_interval = max(1, len(monthly_df) // 12 if len(monthly_df) > 0 else 1)\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=locator_interval))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21187ba8",
   "metadata": {},
   "source": [
    "### Function 2.E: Plotting Individual Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c91eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_single_index(index_series, title='Index Performance', y_limits=None):\n",
    "    \"\"\"\n",
    "    Plots a single index on its own monthly chart.\n",
    "    If y_limits (a tuple) is provided, it sets a fixed y-axis range.\n",
    "    \"\"\"\n",
    "    if index_series.empty or index_series.dropna().empty:\n",
    "        print(f\"Index series '{title}' is empty or all NaN after date filtering. Cannot plot.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        monthly_series = index_series.resample('ME').last().dropna()\n",
    "    except AttributeError:\n",
    "        monthly_series = index_series.resample('M').last().dropna()\n",
    "\n",
    "    if monthly_series.empty:\n",
    "        print(f\"No data available for monthly plotting for '{title}'.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(monthly_series.index, monthly_series.values, marker='o', linestyle='-', label=title)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Index Value')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    \n",
    "    # This is the new part: Apply the standardized y-axis limits if they are provided.\n",
    "    if y_limits:\n",
    "        plt.ylim(y_limits)\n",
    "        \n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    locator_interval = max(1, len(monthly_series) // 12 if len(monthly_series) > 0 else 1)\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=locator_interval))\n",
    "    plt.gcf().autofmt_xdate()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb55615",
   "metadata": {},
   "source": [
    "## Part 3: Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabba46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Script ---\n",
    "\n",
    "# 1. DEFINE YOUR PARAMETERS HERE\n",
    "file_path = \"calculated_indices.xlsx\"\n",
    "sheet_names = [\"Neo Bank Index\", \"Challenger Bank Index\", \"Traditional Bank Index\"]\n",
    "hard_stop_date = pd.to_datetime('2025-05-01')\n",
    "\n",
    "print(f\"--- Loading pre-calculated indices from: {file_path} ---\")\n",
    "\n",
    "# 2. Load each index\n",
    "loaded_indices = []\n",
    "try:\n",
    "    for sheet in sheet_names:\n",
    "        index_series = pd.read_excel(file_path, sheet_name=sheet, index_col=0, parse_dates=True).iloc[:, 0]\n",
    "        index_series.name = sheet\n",
    "        loaded_indices.append(index_series)\n",
    "        print(f\"Successfully loaded sheet: '{sheet}'\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nERROR: The file was not found at the specified path.\")\n",
    "    exit()\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while reading the Excel file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 3. Proceed only if data was loaded\n",
    "if not loaded_indices or len(loaded_indices) < 2:\n",
    "    print(\"\\n--- At least two indices are required for comparison. Script finished. ---\")\n",
    "else:\n",
    "    # --- Part 1: Generate the Comparative Chart ---\n",
    "    temp_df = pd.concat(loaded_indices, axis=1)\n",
    "    first_common_date = temp_df.dropna().index[0]\n",
    "    print(f\"\\n--- Aligning all indices to first common valid date: {first_common_date.date()} ---\")\n",
    "    print(f\"--- Truncating all data to end on or before: {hard_stop_date.date()} ---\")\n",
    "\n",
    "    final_slice_df = temp_df.loc[first_common_date:hard_stop_date]\n",
    "    comparison_df = (final_slice_df / final_slice_df.iloc[0]) * 100\n",
    "\n",
    "    print(\"\\n--- Aligned and Re-based Index Comparison (First 5 rows) ---\")\n",
    "    print(comparison_df.head())\n",
    "    plot_multiple_indices(comparison_df, title=f'Comparative Index Performance (from pre-calculated file)')\n",
    "\n",
    "    # --- Part 2: Generate Individual Charts with Standardized Y-Axis ---\n",
    "    print(f\"\\n--- Generating individual plots from {first_common_date.date()} to {hard_stop_date.date()} ---\")\n",
    "    \n",
    "    # NEW: Calculate the common y-axis range BEFORE plotting.\n",
    "    # 1. Get the slice of data that will be plotted.\n",
    "    individual_plots_df = temp_df.loc[first_common_date:hard_stop_date]\n",
    "    \n",
    "    # 2. Find the absolute min and max across all data in that slice.\n",
    "    global_min = individual_plots_df.min().min()\n",
    "    global_max = individual_plots_df.max().max()\n",
    "    \n",
    "    # 3. Add 5% padding for better visualization.\n",
    "    padding = (global_max - global_min) * 0.05\n",
    "    y_axis_limits = (global_min - padding, global_max + padding)\n",
    "    \n",
    "    print(f\"Standardizing individual plot Y-axis from {y_axis_limits[0]:.2f} to {y_axis_limits[1]:.2f}\")\n",
    "\n",
    "    # Loop through the columns of our sliced dataframe.\n",
    "    for index_name in individual_plots_df.columns:\n",
    "        # Pass the calculated y_axis_limits to the plotting function.\n",
    "        plot_single_index(\n",
    "            individual_plots_df[index_name], \n",
    "            title=f\"{index_name} (from common start date)\",\n",
    "            y_limits=y_axis_limits\n",
    "        )\n",
    "        \n",
    "print(\"\\n--- Script Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12322aca",
   "metadata": {},
   "source": [
    "## Saving the equal/aligned indexes to excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439141b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. DEFINE THE OUTPUT FILENAME\n",
    "output_file_name = 'calculated_indices_EQUAL.xlsx'\n",
    "\n",
    "# 2. CHECK IF THE RE-BASED DATA EXISTS\n",
    "# We use 'comparison_df' which contains the data re-based to 100.\n",
    "if 'comparison_df' in locals() and not comparison_df.empty:\n",
    "    print(f\"\\n--- Saving re-based index data (starts at 100) to: {output_file_name} ---\")\n",
    "    \n",
    "    try:\n",
    "        # Use pd.ExcelWriter to save multiple sheets to one file.\n",
    "        with pd.ExcelWriter(output_file_name, engine='openpyxl') as writer:\n",
    "            \n",
    "            # Loop through each column (each index) in the comparison DataFrame.\n",
    "            for index_name in comparison_df.columns:\n",
    "                \n",
    "                # Select the data for the current index.\n",
    "                single_index_df = comparison_df[[index_name]]\n",
    "                \n",
    "                # Write this single-column DataFrame to a new sheet.\n",
    "                single_index_df.to_excel(writer, sheet_name=index_name, index=True)\n",
    "                \n",
    "                print(f\"  - Saved '{index_name}' to its own sheet.\")\n",
    "\n",
    "        print(f\"\\nSuccessfully created '{output_file_name}'. Each index starts at 100.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nERROR: Could not save the Excel file. Reason: {e}\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\n--- No final re-based data available to save. Skipping file export. ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b995878c",
   "metadata": {},
   "source": [
    "# Part 4: Stationary Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288023b3",
   "metadata": {},
   "source": [
    "## Function 4.A: Running ADF and KPSS on the 3 indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebad7163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_stationarity_tests(series, series_name):\n",
    "    \"\"\"\n",
    "    Runs and interprets the Augmented Dickey-Fuller and KPSS tests on a time series.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Stationarity Tests for {series_name} ---\")\n",
    "\n",
    "    # Drop missing values for the tests\n",
    "    series_cleaned = series.dropna()\n",
    "\n",
    "    # --- Augmented Dickey-Fuller Test ---\n",
    "    # The ADF test checks for non-stationarity.\n",
    "    # Null Hypothesis (H0): The series has a unit root (it is non-stationary).\n",
    "    # Alternative Hypothesis (HA): The series does not have a unit root (it is stationary).\n",
    "    print(\"\\n1. Augmented Dickey-Fuller Test (ADF):\")\n",
    "    # The adfuller function returns several values; we're primarily interested in the p-value.\n",
    "    adf_result = adfuller(series_cleaned)\n",
    "    p_value_adf = adf_result[1]\n",
    "\n",
    "    print(f\"   ADF Statistic: {adf_result[0]}\")\n",
    "    print(f\"   p-value: {p_value_adf}\")\n",
    "    print(\"   Critical Values:\")\n",
    "    for key, value in adf_result[4].items():\n",
    "        print(f'      {key}: {value}')\n",
    "\n",
    "    if p_value_adf <= 0.05:\n",
    "        print(\"\\n   Conclusion: Strong evidence against the null hypothesis (p <= 0.05).\")\n",
    "        print(\"   Reject the null hypothesis. The series is likely stationary.\")\n",
    "    else:\n",
    "        print(\"\\n   Conclusion: Weak evidence against the null hypothesis (p > 0.05).\")\n",
    "        print(\"   Fail to reject the null hypothesis. The series is likely non-stationary.\")\n",
    "\n",
    "    # --- KPSS Test ---\n",
    "    # The KPSS test checks for stationarity around a mean or trend.\n",
    "    # Null Hypothesis (H0): The series is stationary around a constant (trend-stationary).\n",
    "    # Alternative Hypothesis (HA): The series has a unit root (it is not stationary).\n",
    "    print(\"\\n2. Kwiatkowski-Phillips-Schmidt-Shin Test (KPSS):\")\n",
    "    # We use 'ct' to test for stationarity around a trend, which is common for financial indices.\n",
    "    kpss_result = kpss(series_cleaned, regression='ct')\n",
    "    p_value_kpss = kpss_result[1]\n",
    "\n",
    "    print(f\"   KPSS Statistic: {kpss_result[0]}\")\n",
    "    print(f\"   p-value: {p_value_kpss}\")\n",
    "    print(\"   Critical Values:\")\n",
    "    for key, value in kpss_result[3].items():\n",
    "        print(f'      {key}: {value}')\n",
    "\n",
    "    if p_value_kpss <= 0.05:\n",
    "        print(\"\\n   Conclusion: Strong evidence against the null hypothesis (p <= 0.05).\")\n",
    "        print(\"   Reject the null hypothesis. The series is likely non-stationary.\")\n",
    "    else:\n",
    "        print(\"\\n   Conclusion: Weak evidence against the null hypothesis (p > 0.05).\")\n",
    "        print(\"   Fail to reject the null hypothesis. The series is likely trend-stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a8e33c",
   "metadata": {},
   "source": [
    "# Part 5: Main Script for ADF and KPSS tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438252df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Script to Load and Test Indices ---\n",
    "excel_file_path = 'calculated_indices_EQUAL.xlsx'\n",
    "try:\n",
    "    # Load each sheet into a separate DataFrame\n",
    "    neo_index_df = pd.read_excel(excel_file_path, sheet_name='Neo Bank Index', index_col=0)\n",
    "    challenger_index_df = pd.read_excel(excel_file_path, sheet_name='Challenger Bank Index', index_col=0)\n",
    "    traditional_index_df = pd.read_excel(excel_file_path, sheet_name='Traditional Bank Index', index_col=0)\n",
    "\n",
    "    # Extract the series from the DataFrames\n",
    "    neo_series = neo_index_df.iloc[:, 0]\n",
    "    challenger_series = challenger_index_df.iloc[:, 0]\n",
    "    traditional_series = traditional_index_df.iloc[:, 0]\n",
    "\n",
    "    # Run tests for each index\n",
    "    run_stationarity_tests(neo_series, \"Neo Bank Index\")\n",
    "    run_stationarity_tests(challenger_series, \"Challenger Bank Index\")\n",
    "    run_stationarity_tests(traditional_series, \"Traditional Bank Index\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{excel_file_path}' was not found.\")\n",
    "    print(\"Please make sure you have run the first code block to generate it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Step 1: Applying First-Difference Transformation ---\")\n",
    "# The .diff() method calculates the difference from the previous day.\n",
    "# The .dropna() method removes the first row, which is now NaN.\n",
    "try:\n",
    "    neo_diff = neo_series.diff().dropna()\n",
    "    challenger_diff = challenger_series.diff().dropna()\n",
    "    traditional_diff = traditional_series.diff().dropna()\n",
    "    print(\"Transformation complete. The new series represent daily returns.\")\n",
    "except NameError:\n",
    "    print(\"Error: The original series (e.g., neo_series) were not found.\")\n",
    "    print(\"Please make sure you have run the previous code cell first.\")\n",
    "\n",
    "\n",
    "def run_stationarity_tests_on_transformed_data(series, series_name):\n",
    "    \"\"\"\n",
    "    Runs ADF and KPSS tests specifically for transformed (differenced) data.\n",
    "    The key difference is using regression='c' in the KPSS test, as the trend has been removed.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Stationarity Tests for: {series_name} ---\")\n",
    "    \n",
    "    # --- Augmented Dickey-Fuller Test ---\n",
    "    # Null Hypothesis: The series is NON-STATIONARY.\n",
    "    # We expect to REJECT this for differenced data.\n",
    "    print(\"\\n1. Augmented Dickey-Fuller Test (ADF):\")\n",
    "    adf_result = adfuller(series)\n",
    "    p_value_adf = adf_result[1]\n",
    "    print(f\"   p-value: {p_value_adf}\")\n",
    "    if p_value_adf <= 0.05:\n",
    "        print(\"   Conclusion: REJECT the null hypothesis. The series is likely STATIONARY.\")\n",
    "    else:\n",
    "        print(\"   Conclusion: FAIL to reject the null hypothesis. The series is likely NON-STATIONARY.\")\n",
    "\n",
    "    # --- KPSS Test ---\n",
    "    # Null Hypothesis: The series is STATIONARY.\n",
    "    # We expect to FAIL TO REJECT this for differenced data.\n",
    "    print(\"\\n2. Kwiatkowski-Phillips-Schmidt-Shin Test (KPSS):\")\n",
    "    # Using regression='c' because we are testing for stationarity around a constant (level), not a trend.\n",
    "    kpss_result = kpss(series, regression='c')\n",
    "    p_value_kpss = kpss_result[1]\n",
    "    print(f\"   p-value: {p_value_kpss}\")\n",
    "    if p_value_kpss <= 0.05:\n",
    "        print(\"   Conclusion: REJECT the null hypothesis. The series is likely NON-STATIONARY.\")\n",
    "    else:\n",
    "        print(\"   Conclusion: FAIL to reject the null hypothesis. The series is likely STATIONARY.\")\n",
    "\n",
    "\n",
    "# --- Step 2: Re-run tests on the NEW differenced data ---\n",
    "# We now call the new testing function on our transformed series.\n",
    "print(\"\\n--- Step 2: Running Stationarity Tests on Transformed (Differenced) Data ---\")\n",
    "\n",
    "run_stationarity_tests_on_transformed_data(neo_diff, \"Neo Bank Index (Transformed)\")\n",
    "run_stationarity_tests_on_transformed_data(challenger_diff, \"Challenger Bank Index (Transformed)\")\n",
    "run_stationarity_tests_on_transformed_data(traditional_diff, \"Traditional Bank Index (Transformed)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
